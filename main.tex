\documentclass[12pt]{article}

%DEFINIZIONE DEI PACCHETTI GENERICI
\usepackage[a4paper,top=3.5cm,bottom=2.5cm,left=3cm,right=3.5cm]{geometry}
\usepackage{times}
\usepackage{titlesec}
%\usepackage{lipsum}
\usepackage{titletoc}

%DEFINIZIONE DEI PACCHETTI MATEMATICI
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

%STILE DELLE PROPOSIZIONI
\theoremstyle{plain}

% Intestazioni in ingelse
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{prob}[thm]{Problem}
\newtheorem*{quest*}{Question}
\newtheorem*{notat*}{Notation}
\newtheorem{alg}{Algorithm}

% Intestazioni in italiano
%\newtheorem{thm}{Teorema}[section]
%\newtheorem{lem}[thm]{Lemma}
%\newtheorem{prop}[thm]{Proposizione}
%\newtheorem{cor}[thm]{Corollario}
%\newtheorem{defn}[thm]{Definizione}
%\newtheorem{rmk}[thm]{Osservazione}
%\newtheorem{ex}[thm]{Esempio}
%\newtheorem{prob}[thm]{Problema}
%\newtheorem*{notat*}{Notazione}
%\newtheorem*{quest*}{Domanda}

%STILE DELLE SEZIONI
\titleformat{\section}
{\normalfont \scshape \centering}{\thesection.}{0,5em}{}
%\titlespacing*{\section}{0pt}{50pt}{0.5cm}

\titleformat{\subsection}
{\normalfont \bfseries }{\normalfont \thesubsection.}{0,5em}{}
%\titlespacing*{\subsection}{0pt}{50pt}{0.5cm}

\titlecontents{section}[0cm]{}
{\normalfont \thecontentslabel. \enspace}
{\hspace*{-5.3em}}
{ \hfill \normalfont \contentspage}

\titlecontents{subsection}[0cm]{}
{\normalfont \thecontentslabel. \enspace}
{\hspace*{-5.3em}}
{ \hfill \normalfont \contentspage}

%OPZIONI PER LA BIBLIOGRAFIA
\usepackage[
%backend=bibtex,
backend=biber,
style=alphabetic,
]{biblatex}
\addbibresource{bibliography.bib}
\AtNextBibliography{\small}

% imposto lo stile dell'abstract
\renewcommand{\abstractname}{\normalfont \scshape \centering Abstract}

%INIZIO DEL DOCUMENTO
\begin{document}

    % impostazione del titolo
	\begin{center}
	    \fontsize{12pt}{0pt} % dimensioni del titolo
        \textbf{NEURAL NETWORKS FOUNDATIONS} % titolo
	\end{center}

    %impostazione dell'indice
    {
    \fontsize{12pt}{0pt} % dimensioni delle voci
    \tableofcontents % print della tavola dei contenuti
    }

    \fontsize{12pt}{0pt} % dimensione del font e spaziatura tra le righe
    %INIZIA A SCRIVERE QUI

    \section{Some linear algebra}
    In this chapter we are going to consider only the real numbers $\mathbb{R}$.

    \begin{defn}
        Let $f : \mathbb{R} \longrightarrow \mathbb{R}$, we say that $f$ is a linear function if
        \[ f(\lambda \cdot x) = \lambda f(x), \forall \lambda, x \in \mathbb{R}\]
        and
        \[ f(x + y) = f(x) + f(y), \forall x, y \in \mathbb{R}.\]
    \end{defn}

    \begin{ex}
        Consider $a \in \mathbb{R}$, and define $f(x) = a \cdot x$, then $f$ is linear.
        Indeed consider $x, \lambda \in \mathbb{R}$, then
        \[ f(\lambda x) = a \cdot (\lambda x) = (a \lambda) x = \lambda a x,\]
        and given $x, y \in \mathbb{R}$ we have that
        \[ f(x+y) = \lambda(x + y) = \lambda x + \lambda y = f(x) + f(y).\]
    \end{ex}

    In particular it holds.
    \begin{lem} \label{lem:linearfun}
        For every linear map $f : \mathbb{R} \longrightarrow \mathbb{R}$, there exists $a \in \mathbb{R}$ such that
        \[ f(x) = a \cdot x.\]
    \end{lem}

    We now introduce a new parameter.
    \begin{defn}
        Consider a collection of linear function $\{f_a : \mathbb{R} \longrightarrow \mathbb{R}\}_{a \in \mathbb{R}}$. We call this collection a 1-parameter group of linear functions if 
        \[ \lambda f_a(x) = f_{\lambda a}(x),\]
        and
        \[ f_a (x) + f_b(x) = f_{a+b}(x).\]
    \end{defn}

    \begin{rmk}
        Not every collection of linear function $\{f_a \}_{a\in \mathbb{R}}$ is a 1-parameter group of linear functions. Indeed, consider the functions $f_a(x) = c\cdot x$ for every $a \in \mathbb{R}$. Then
        \[ f_a(x) + f_b(x) = cx + cx = 2cx \neq cx = f_{a+b}(x).\]
    \end{rmk}

    \begin{ex} \label{ex:f_a(x)=ax}
        Consider $f_a(x) = ax$. Then $\{f_a\}_{a\in \mathbb{R}}$ is a one parameter group of linear functions. Indeed:
        \[ f_a(x) + f_b(x) = ax + bx = (a+b)x = f_{a+b}(x),\]
        and
        \[ \lambda f_a(x) = \lambda a x = f_{\lambda a}(x).\]
    \end{ex}

    \subsection{Norms}
    Given $\mathbb{R}^n$, we define a particular new type of function.

    \begin{defn}
        A function $N : \mathbb{R}^n \longrightarrow \mathbb{R}^n$, is called a norm if
        \begin{enumerate}
            \item[(N1)] $N(x) = 0 \implies x =0$;
            \item[(N2)] $N(x) \ge 0, \, \forall x \in \mathbb{R}^n$;
            \item[(N2)] $N(x+y) \le N(x) + N(y), \, \forall x, y \in \mathbb{R}^n$;
            \item[(N3)] $N(\lambda x) = |\lambda| N(x), \, \forall \lambda \in \mathbb{R}, x \in \mathbb{R}^n$.
        \end{enumerate}
    \end{defn}

    \begin{ex}
        An example of norm is the euclidean norm. Given $x \in \mathbb{R}^n$, we define
        \[ \Vert x \Vert := \sqrt{\sum_{i=1}^n x_i^2}.\]
        This norm represents the distance between a point $x \in \mathbb{R}^n$ and the origin.
    \end{ex}
    
    \section{Linear neurons}
    
    \begin{defn}
        A simple linear neuron, $n$, is a function
        \[ n_a : \mathbb{R} \longrightarrow \mathbb{R}\]
        such that $n_a(x) = a x $, and $a \in \mathbb{R}$.
        The number $a$ is called parameter.
        We will refer to this kind of neuron with SLN.
    \end{defn}

    \begin{rmk}
        By Example \ref{ex:f_a(x)=ax}, a simple linear neuron is a 1-parameter group of linear functions.
    \end{rmk}

    Technically speaking given a SLN, $n$, this is already a neural network. However we will see in the following that this type of neuron is not very interesting even if we can use it in some simple cases.

    But for now we will try to work with this type of neuron and see what we can do. To work with a neural network, a neuron in this case, we need to introduce the notion of backpropagation.
    
    \subsection{Backpropagation with a SLN}
    Consider an SLN $n$. Consider a set of data $\{ (x_i, y_i) \}_{i =1, \dots, n}$. Backpropagation is a method used to find a parameter $\bar{t} \in \mathbb{R}$, so that the function $n_{\bar{t}}$ approximate the set of data $\{ (x_i, y_i) \}_{i =1, \dots, n}$, i.e.
    \[ n_{\bar{t}}(x_i) \approx y_i, \, \forall i = 1, \dots, n.\]

    Consider and initial parameter $a \in \mathbb{R}$. So we have an inital approximation of the set of data give by the function $n_a$.
    
    The first thing we want to introduce is the cost function.
    \begin{defn}
        Given an SLN $\{ n_a \}_{a \in \mathbb{R}}$, and a set of data $\{ (x_i, y_i) \}_{i =1, \dots, n}$. We define the cost function
        \[ C(t) := \sum_{i=1}^n (n_t(x_i) - y_i)^2.\]
    \end{defn}

    \begin{rmk}
        The cost function $C(t)$ can be described also using the euclidean norm.
        Define the vectors
        \[ v(t) = \left ( \begin{matrix}
            n_t(x_1) \\
            n_t(x_2) \\
            \vdots \\
            n_t(x_n)
        \end{matrix}  \right ) \hspace{1cm}
        y = \left ( \begin{matrix}
            y_1\\
            y_2\\
            \vdots\\
            y_n
        \end{matrix} \right ).\]
        Then the cost function can be define as
        \[ C(t) = \Vert v(t) - y \Vert^2.\]
    \end{rmk}

    So, to find the best approximation of our set of data, we simply needs to minimize the cost function $C(a)$. Indeed, suppose that $m \in \mathbb{R}$ realize the minimum of $C$, i.e.
    \[ C(m) = \min_{a \in \mathbb{R}} C(a).\]
    Therefore the vector $v(m)$ is the vector that best approximates the set of data.

    To minimize the cost function $C$, we need to know how $C$ changes with respect to the parameter $t$ when the chosen parameter is $a$, and we do this by computing $\frac{\partial C}{\partial t}(a)$. Once we have the derivative of $C$ in $a$, we can analyze it:
    \begin{enumerate}
        \item[(1)] if $\frac{\partial C}{\partial t}(a) > 0$, then on the increasing of $a$ the cost will increase. Thus, in order to decrease the cost, we need to decrease the parameter, and our new parameter will be
        \[ a' = a - \frac{\partial C}{\partial t}(a). \]
        \item[(2)] if $\frac{\partial C}{\partial t}(a) < 0$, then on the increasing of $a$ the cost will decrease. Thus, in order to decrease the cost, we need to increase the parameter, and our new parameter will be
        \[ a' = a - \frac{\partial C}{\partial t}(a). \]
    \end{enumerate}

    Now we have a new parameter $a'$, such that the vector $v(a')$ is a better approximation than $v(a)$ of the set of data.
    The backtracking is the algorithm that repeats this process a fixed number of time $N \in \mathbb{N}$.

    We give here the complete algorithm.
    \begin{alg}[Backtracking, version 1]\ \\
        Given $N \in \mathbb{N}$, $a \in \mathbb{R}$, an SLN $n$.\\
        Define $a_1 = a$;\\
        For $i = 2, \dots, N$:\\
        \par\hspace{0.6cm}Compute $\frac{\partial C}{\partial t}(a_{i-1})$;\\
        \par\hspace{0.6cm}Define $a_i = a_{i-1} - \frac{\partial C}{\partial t}(a_{i-1})$;\\
    $a_N$ is the final approximation.        
    \end{alg}
    
    %STAMPA DELLA BIBLIOGRAFIA
	\printbibliography[title=References and Bibliography]
	
\end{document}